{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "[]\n",
      "Num GPUs Available:  0\n",
      "Num CPUs Available:  1\n",
      "Num TPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, GlobalAveragePooling1D, Normalization, Multiply\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"Num TPUs Available: \", len(tf.config.list_physical_devices('TPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell performs data preprocessing on a CSV file located at the specified file path. Here's a breakdown of the preprocessing steps:\n",
    "\n",
    "1. **Reading Data**: Reads the CSV file into a pandas DataFrame named `whole_data`.\n",
    "2. **Removing Columns**: Drops specific columns ('website', 'place_links', 'description', 'territory_id') from the DataFrame.\n",
    "3. **Converting Tag Data**: Defines a function `safe_int_convert` to convert tag data into a standardized format. Tags are either stored as lists, single integers, or comma-separated strings. This function converts them into a list of integers.\n",
    "4. **Applying Data Transformation**: Applies the `safe_int_convert` function to the 'tags' column of the DataFrame. Converts the 'rating' column to float type and fills missing values with the mean.\n",
    "5. **Padding Sequences**: Utilizes `pad_sequences` from Keras to pad sequences of tags to ensure uniform length.\n",
    "6. **Extracting Unique Tags**: Flattens the padded tags and extracts unique tags.\n",
    "7. **Return Values**: Returns the preprocessed DataFrame `whole_data` and an array of unique tags `unique_tags`.\n",
    "\n",
    "Additionally, it prints information about the preprocessed data, such as the shape of the DataFrame and the unique tags extracted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "\n",
      "Whole data shape:  (187, 5)\n",
      "\n",
      "Unique tags shape:  (23,)\n",
      "Unique tags:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
      "\n",
      "Whole data columns:  Index(['name', 'rating', 'place_id', 'tags', 'locationYX'], dtype='object')\n",
      "\n",
      "Whole data table: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>place_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>locationYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kadikoy Ferry Terminal</td>\n",
       "      <td>4.6</td>\n",
       "      <td>370509</td>\n",
       "      <td>[1, 9, 14, 0, 0, 0, 0]</td>\n",
       "      <td>40.99269778351916,29.023280555674663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kadikoy Bull Statue</td>\n",
       "      <td>4.5</td>\n",
       "      <td>761356</td>\n",
       "      <td>[1, 3, 6, 0, 0, 0, 0]</td>\n",
       "      <td>40.990473264783475,29.029131932189433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kadikoy-moda Streets</td>\n",
       "      <td>4.6</td>\n",
       "      <td>877687</td>\n",
       "      <td>[13, 14, 15, 0, 0, 0, 0]</td>\n",
       "      <td>41.0300084184215,28.98441527977153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moda Beach Park</td>\n",
       "      <td>4.7</td>\n",
       "      <td>292410</td>\n",
       "      <td>[9, 11, 12, 0, 0, 0, 0]</td>\n",
       "      <td>40.98000940235465,29.026848556608424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IDEA Kadikoy</td>\n",
       "      <td>4.6</td>\n",
       "      <td>827370</td>\n",
       "      <td>[9, 14, 16, 0, 0, 0, 0]</td>\n",
       "      <td>40.98015752580784,29.02810950338273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Palace of the Porphyrogenitus</td>\n",
       "      <td>4.5</td>\n",
       "      <td>871687</td>\n",
       "      <td>[2, 3, 4, 7, 0, 0, 0]</td>\n",
       "      <td>41.03788124723142,28.93945445055248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Column of Marcian</td>\n",
       "      <td>4.3</td>\n",
       "      <td>265458</td>\n",
       "      <td>[1, 2, 3, 7, 0, 0, 0]</td>\n",
       "      <td>41.02630809319681,28.95342213409043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Beyazıt Tower</td>\n",
       "      <td>4.5</td>\n",
       "      <td>505870</td>\n",
       "      <td>[1, 2, 3, 7, 0, 0, 0]</td>\n",
       "      <td>41.01817417223928,28.96347312671263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Grand Bazaar</td>\n",
       "      <td>4.1</td>\n",
       "      <td>102220</td>\n",
       "      <td>[1, 2, 3, 7, 9, 17, 0]</td>\n",
       "      <td>41.014800403046365,28.967378072061717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Ruins of Philanthropos Church</td>\n",
       "      <td>4.5</td>\n",
       "      <td>821293</td>\n",
       "      <td>[1, 2, 3, 5, 0, 0, 0]</td>\n",
       "      <td>41.02291936064259,28.984498453867523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  rating  place_id  \\\n",
       "0            Kadikoy Ferry Terminal     4.6    370509   \n",
       "1               Kadikoy Bull Statue     4.5    761356   \n",
       "2              Kadikoy-moda Streets     4.6    877687   \n",
       "3                  Moda Beach Park      4.7    292410   \n",
       "4                      IDEA Kadikoy     4.6    827370   \n",
       "..                              ...     ...       ...   \n",
       "182   Palace of the Porphyrogenitus     4.5    871687   \n",
       "183               Column of Marcian     4.3    265458   \n",
       "184                   Beyazıt Tower     4.5    505870   \n",
       "185                    Grand Bazaar     4.1    102220   \n",
       "186   Ruins of Philanthropos Church     4.5    821293   \n",
       "\n",
       "                         tags                             locationYX  \n",
       "0      [1, 9, 14, 0, 0, 0, 0]   40.99269778351916,29.023280555674663  \n",
       "1       [1, 3, 6, 0, 0, 0, 0]  40.990473264783475,29.029131932189433  \n",
       "2    [13, 14, 15, 0, 0, 0, 0]     41.0300084184215,28.98441527977153  \n",
       "3     [9, 11, 12, 0, 0, 0, 0]   40.98000940235465,29.026848556608424  \n",
       "4     [9, 14, 16, 0, 0, 0, 0]    40.98015752580784,29.02810950338273  \n",
       "..                        ...                                    ...  \n",
       "182     [2, 3, 4, 7, 0, 0, 0]    41.03788124723142,28.93945445055248  \n",
       "183     [1, 2, 3, 7, 0, 0, 0]    41.02630809319681,28.95342213409043  \n",
       "184     [1, 2, 3, 7, 0, 0, 0]    41.01817417223928,28.96347312671263  \n",
       "185    [1, 2, 3, 7, 9, 17, 0]  41.014800403046365,28.967378072061717  \n",
       "186     [1, 2, 3, 5, 0, 0, 0]   41.02291936064259,28.984498453867523  \n",
       "\n",
       "[187 rows x 5 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "def preprocess_data(file_path):\n",
    "    whole_data = pd.read_csv(file_path)\n",
    "    whole_data.drop([\"website\", \"place_links\", \"description\", \"territory_id\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    def safe_int_convert(tag_list):\n",
    "        if isinstance(tag_list, list):\n",
    "            return tag_list\n",
    "        elif isinstance(tag_list, (int, float)):\n",
    "            return [int(tag_list)]\n",
    "        elif isinstance(tag_list, str):\n",
    "            try:\n",
    "                return list(map(int, tag_list.split(',')))\n",
    "            except ValueError:\n",
    "                return []\n",
    "        return []\n",
    "\n",
    "\n",
    "    whole_data['tags'] = whole_data['tags'].apply(safe_int_convert)\n",
    "    whole_data['rating'] = whole_data['rating'].astype(float).fillna(whole_data['rating'].mean())\n",
    "    tags_padded = pad_sequences(whole_data['tags'], padding='post')\n",
    "    whole_data['tags'] = list(tags_padded)\n",
    "    tags_flat = [tag for sublist in whole_data['tags'].tolist() for tag in sublist]\n",
    "    unique_tags = np.unique(tags_flat)\n",
    "    \n",
    "    return whole_data, unique_tags\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "whole_data, unique_tags = preprocess_data('../../../data/place_data.csv')\n",
    "\n",
    "print(\"\\nWhole data shape: \", whole_data.shape)\n",
    "\n",
    "print(\"\\nUnique tags shape: \", unique_tags.shape)\n",
    "print(\"Unique tags: \", unique_tags)\n",
    "\n",
    "print(\"\\nWhole data columns: \", whole_data.columns)\n",
    "\n",
    "print(\"\\nWhole data table: \")\n",
    "whole_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a custom triplet loss function commonly used in triplet loss networks for learning embeddings. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `anchor`: The embedding vector for the anchor sample.\n",
    "   - `positive`: The embedding vector for the positive sample (same class as anchor).\n",
    "   - `negative`: The embedding vector for the negative sample (different class from anchor).\n",
    "   - `margin`: The margin value, a hyperparameter that specifies the minimum difference between the distances of anchor-positive and anchor-negative pairs.\n",
    "\n",
    "2. **Calculating Distances**:\n",
    "   - Computes the Euclidean distance between the anchor and positive samples, and between the anchor and negative samples.\n",
    "   - Uses `tf.reduce_sum` to sum the squared differences along the last axis (feature dimension).\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "   - Computes the difference between the positive and negative distances, adds the margin, and takes the element-wise maximum with 0. This ensures that the loss is only calculated when the positive distance is not significantly smaller than the negative distance.\n",
    "   - Finally, computes the mean of these losses across all samples.\n",
    "\n",
    "4. **Return Value**:\n",
    "   - Returns the mean triplet loss computed over the entire batch of samples.\n",
    "\n",
    "This loss function encourages the network to learn embeddings such that the distance between the anchor and positive samples is minimized, while the distance between the anchor and negative samples is maximized by at least the specified margin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Triplet Loss\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    positive_distance = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "    negative_distance = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "    loss = tf.maximum(positive_distance - negative_distance + margin, 0.0)\n",
    "    return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a function to create a triplet model for learning embeddings. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `unique_tags`: An array containing unique tag IDs used for embedding the tag input.\n",
    "   - `embedding_dim`: Dimensionality of the embedding space.\n",
    "   - `dense_units`: Number of units in the dense layer.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - **Tag Input**: Defines an input layer for tag data.\n",
    "   - **Tag Lookup**: Utilizes `IntegerLookup` layer from TensorFlow to map tag IDs to embedding indices.\n",
    "   - **Tag Embedding**: Embeds the tag input using an `Embedding` layer with specified dimensions.\n",
    "   - **Tag Pooling**: Uses `GlobalAveragePooling1D` to aggregate tag embeddings across time dimension.\n",
    "   - **Rating Input**: Defines an input layer for the rating data.\n",
    "   - **Rating Normalization**: Normalizes the rating input using `Normalization` layer.\n",
    "   - **Combining Inputs**: Concatenates the pooled tag embeddings and normalized rating.\n",
    "   - **Dense Layers**: Applies a dense layer with ReLU activation to the combined embeddings.\n",
    "   - **Output Layer**: Outputs the embeddings with dimensions equal to `embedding_dim`.\n",
    "\n",
    "3. **Model Creation**:\n",
    "   - Constructs a `Model` object with inputs as tag and rating inputs and output as the embedding layer.\n",
    "\n",
    "This model architecture combines both tag embeddings and normalized ratings to generate embeddings of the specified dimensionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Triplet Model\n",
    "def create_triplet_model(unique_tags, embedding_dim=64, dense_units=128):\n",
    "    tag_input = Input(shape=(None,), dtype=tf.int32, name='tags')\n",
    "    tag_lookup = tf.keras.layers.IntegerLookup(vocabulary=unique_tags, mask_token=None)\n",
    "    tag_embedding = Embedding(input_dim=len(unique_tags) + 1, output_dim=embedding_dim)\n",
    "    tag_embeddings = tag_embedding(tag_lookup(tag_input))\n",
    "    tag_embeddings = GlobalAveragePooling1D()(tag_embeddings)\n",
    "    \n",
    "    rating_input = Input(shape=(1,), dtype=tf.float32, name='rating')\n",
    "    rating_normalization = Normalization(axis=None)\n",
    "    rating_normalization.build((None, 1))\n",
    "    rating_normalized = rating_normalization(rating_input)\n",
    "    \n",
    "    combined_embeddings = Concatenate()([tag_embeddings, rating_normalized])\n",
    "    dense = Dense(dense_units, activation='relu')(combined_embeddings)\n",
    "    output = Dense(embedding_dim)(dense)\n",
    "    \n",
    "    model = Model(inputs=[tag_input, rating_input], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a function to generate triplets for training a triplet loss model. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameter**:\n",
    "   - `dataframe`: Pandas DataFrame containing the preprocessed data.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - Initializes dictionaries to store features for anchor, positive, and negative samples.\n",
    "   - Iterates over each row in the provided DataFrame.\n",
    "\n",
    "3. **Generating Triplets**:\n",
    "   - For each row, selects a random index from the DataFrame to create positive and negative samples.\n",
    "   - Retrieves the corresponding rows for positive and negative samples.\n",
    "   - Appends tag features and rating to the respective dictionaries for anchor, positive, and negative samples.\n",
    "\n",
    "4. **Data Padding**:\n",
    "   - Applies padding to tag sequences using `pad_sequences` to ensure uniform length across all samples.\n",
    "\n",
    "5. **Return Value**:\n",
    "   - Returns three dictionaries containing features for anchor, positive, and negative samples, respectively. Each dictionary contains tag sequences padded to the maximum length and ratings as numpy arrays.\n",
    "\n",
    "This function facilitates the creation of triplets necessary for training a triplet loss model, ensuring that each triplet consists of an anchor, a positive, and a negative sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Triplets for Training\n",
    "def generate_triplets(dataframe):\n",
    "    anchor_features = {'tags': [], 'rating': []}\n",
    "    positive_features = {'tags': [], 'rating': []}\n",
    "    negative_features = {'tags': [], 'rating': []}\n",
    "    \n",
    "    for _, row in dataframe.iterrows():\n",
    "        anchor_features['tags'].append(row['tags'])\n",
    "        anchor_features['rating'].append(row['rating'])\n",
    "        \n",
    "        positive_idx = np.random.choice(dataframe.index)\n",
    "        negative_idx = np.random.choice(dataframe.index)\n",
    "        \n",
    "        positive_row = dataframe.loc[positive_idx]\n",
    "        negative_row = dataframe.loc[negative_idx]\n",
    "        \n",
    "        positive_features['tags'].append(positive_row['tags'])\n",
    "        positive_features['rating'].append(positive_row['rating'])\n",
    "        \n",
    "        negative_features['tags'].append(negative_row['tags'])\n",
    "        negative_features['rating'].append(negative_row['rating'])\n",
    "    \n",
    "    return (\n",
    "        {k: pad_sequences(v, padding='post') if k == 'tags' else np.array(v) for k, v in anchor_features.items()},\n",
    "        {k: pad_sequences(v, padding='post') if k == 'tags' else np.array(v) for k, v in positive_features.items()},\n",
    "        {k: pad_sequences(v, padding='post') if k == 'tags' else np.array(v) for k, v in negative_features.items()}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a generator function for creating batches of triplets during model training. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `anchor_features`: Dictionary containing features for anchor samples.\n",
    "   - `positive_features`: Dictionary containing features for positive samples.\n",
    "   - `negative_features`: Dictionary containing features for negative samples.\n",
    "   - `batch_size`: Number of triplets to generate in each batch.\n",
    "\n",
    "2. **Generator Loop**:\n",
    "   - Enters an infinite loop to continuously generate batches of triplets.\n",
    "   - Shuffles the indices of the anchor features to randomize the order of samples in each epoch.\n",
    "\n",
    "3. **Batch Generation**:\n",
    "   - Divides the shuffled indices into batches of size `batch_size`.\n",
    "   - For each batch, retrieves the corresponding features for anchor, positive, and negative samples.\n",
    "   - Constructs dictionaries for anchor, positive, and negative batches with features.\n",
    "\n",
    "4. **Yielding Batches**:\n",
    "   - Yields a tuple containing dictionaries for anchor, positive, and negative batches.\n",
    "\n",
    "5. **Usage in Training**:\n",
    "   - This generator function can be used with the `fit_generator` method in Keras to train models that require triplets as input.\n",
    "\n",
    "This generator function enables the creation of batches of triplets on-the-fly, allowing efficient training of triplet loss models without the need to store all possible triplets in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet Generator\n",
    "def triplet_generator(anchor_features, positive_features, negative_features, batch_size=32):\n",
    "    while True:\n",
    "        indices = np.arange(len(anchor_features['tags']))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for start in range(0, len(anchor_features['tags']), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_indices = indices[start:end]\n",
    "            \n",
    "            anchor_batch = {k: v[batch_indices] for k, v in anchor_features.items()}\n",
    "            positive_batch = {k: v[batch_indices] for k, v in positive_features.items()}\n",
    "            negative_batch = {k: v[batch_indices] for k, v in negative_features.items()}\n",
    "            \n",
    "            yield (anchor_batch, positive_batch, negative_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a custom training step using TensorFlow's `tf.function` decorator. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `model`: The triplet model to be trained.\n",
    "   - `optimizer`: The optimizer used for updating model weights.\n",
    "   - `anchor_batch`: Dictionary containing features for anchor samples.\n",
    "   - `positive_batch`: Dictionary containing features for positive samples.\n",
    "   - `negative_batch`: Dictionary containing features for negative samples.\n",
    "\n",
    "2. **Training Step**:\n",
    "   - Enters a `tf.GradientTape` context to compute gradients with respect to the model's trainable variables.\n",
    "   - Forward pass: Computes embeddings for anchor, positive, and negative samples using the model.\n",
    "   - Computes the triplet loss using the embeddings.\n",
    "   - Backward pass: Computes gradients of the loss with respect to the model's trainable variables.\n",
    "   - Applies gradients to update the model's weights using the specified optimizer.\n",
    "\n",
    "3. **Return Value**:\n",
    "   - Returns the computed loss for the current training step.\n",
    "\n",
    "4. **Usage in Training Loop**:\n",
    "   - This function can be called within a custom training loop to perform a single optimization step.\n",
    "   - It efficiently computes gradients and updates model weights using the triplet loss.\n",
    "\n",
    "The `@tf.function` decorator ensures that the function is compiled into a TensorFlow graph, optimizing performance during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Training Loop\n",
    "@tf.function\n",
    "def train_step(model, optimizer, anchor_batch, positive_batch, negative_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        anchor_embeddings = model(anchor_batch, training=True)\n",
    "        positive_embeddings = model(positive_batch, training=True)\n",
    "        negative_embeddings = model(negative_batch, training=True)\n",
    "        \n",
    "        loss = triplet_loss(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a function to train a triplet model using a custom training loop. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `model`: The triplet model to be trained.\n",
    "   - `anchor_features`: Dictionary containing features for anchor samples.\n",
    "   - `positive_features`: Dictionary containing features for positive samples.\n",
    "   - `negative_features`: Dictionary containing features for negative samples.\n",
    "   - `epochs`: Number of epochs for training.\n",
    "   - `batch_size`: Number of triplets to include in each training batch.\n",
    "\n",
    "2. **Optimizer Initialization**:\n",
    "   - Initializes an Adam optimizer with a specified learning rate.\n",
    "\n",
    "3. **Triplet Generator Initialization**:\n",
    "   - Initializes a triplet generator using the provided anchor, positive, and negative features and the specified batch size.\n",
    "\n",
    "4. **Training Loop**:\n",
    "   - Iterates over each epoch.\n",
    "   - Within each epoch, iterates over each step in the training data.\n",
    "   - Retrieves the next batch of triplets from the triplet generator.\n",
    "   - Performs a single training step using the `train_step` function defined previously.\n",
    "   - Prints the loss at regular intervals for monitoring training progress.\n",
    "\n",
    "5. **Model Saving**:\n",
    "   - Saves the trained model after completing all epochs.\n",
    "\n",
    "This function facilitates the training of a triplet model using a custom training loop, allowing fine-grained control over the training process, including batch size, optimizer choice, and training duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Triplet Model\n",
    "def train_triplet_model(model, anchor_features, positive_features, negative_features, epochs=100, batch_size=32):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    triplet_gen = triplet_generator(anchor_features, positive_features, negative_features, batch_size=batch_size)\n",
    "    steps_per_epoch = len(anchor_features['tags']) // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step in range(steps_per_epoch):\n",
    "            anchor_batch, positive_batch, negative_batch = next(triplet_gen)\n",
    "            loss = train_step(model, optimizer, anchor_batch, positive_batch, negative_batch)\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    model.save('../.models/triplet_model.keras')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a function to create and train a Neural Collaborative Filtering (NCF) model. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `num_users`: Total number of users in the dataset.\n",
    "   - `num_places`: Total number of places in the dataset.\n",
    "   - `embedding_dim`: Dimensionality of the embedding space for user and place embeddings.\n",
    "   - `hidden_layers`: List of integers specifying the number of units in each hidden layer of the MLP component.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - **User Input**: Defines an input layer for user IDs.\n",
    "   - **User Embedding**: Embeds the user input using an `Embedding` layer.\n",
    "   - **Flattening**: Flattens the user embedding to prepare for concatenation.\n",
    "   - **Place Input**: Defines an input layer for place IDs.\n",
    "   - **Place Embedding**: Embeds the place input using another `Embedding` layer.\n",
    "   - **GMF Component**: Multiplies user and place embeddings element-wise to create a GMF vector.\n",
    "   - **MLP Component**: Concatenates user and place embeddings to create an input vector for the MLP.\n",
    "   - **Hidden Layers**: Applies a series of dense layers with ReLU activation to the concatenated input vector.\n",
    "   - **Combined Vector**: Concatenates the GMF and MLP vectors.\n",
    "   - **Output Layer**: Outputs a single sigmoid value indicating the likelihood of user-place interaction.\n",
    "\n",
    "3. **Model Compilation**:\n",
    "   - Compiles the model using the Adam optimizer and binary cross-entropy loss, with accuracy as the evaluation metric.\n",
    "\n",
    "4. **Return Value**:\n",
    "   - Returns the compiled NCF model ready for training.\n",
    "\n",
    "This function enables the creation of an NCF model architecture suitable for collaborative filtering tasks, with customizable embedding dimensions and hidden layers for the MLP component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Train NCF Model\n",
    "def create_ncf_model(num_users, num_places, embedding_dim=50, hidden_layers=[64, 32, 16, 8]):\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_embedding')(user_input)\n",
    "    user_embedding = Flatten()(user_embedding)\n",
    "    \n",
    "    place_input = Input(shape=(1,), name='place_input')\n",
    "    place_embedding = Embedding(input_dim=num_places, output_dim=embedding_dim, name='place_embedding')(place_input)\n",
    "    place_embedding = Flatten()(place_embedding)\n",
    "    \n",
    "    gmf_vector = Multiply()([user_embedding, place_embedding])\n",
    "    mlp_vector = Concatenate()([user_embedding, place_embedding])\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        mlp_vector = Dense(units, activation='relu')(mlp_vector)\n",
    "        \n",
    "    combined_vector = Concatenate()([gmf_vector, mlp_vector])\n",
    "    output = Dense(1, activation='sigmoid')(combined_vector)\n",
    "    \n",
    "    model = Model(inputs=[user_input, place_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a function to train an NCF model using user-place interaction data. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `model`: The NCF model to be trained.\n",
    "   - `user_data`: Pandas DataFrame containing user-place interaction data, including 'user_index', 'place_index', and 'score' columns.\n",
    "   - `batch_size`: Number of samples per gradient update.\n",
    "   - `epochs`: Number of epochs to train the model.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - Checks if the required columns ('user_index', 'place_index', and 'score') are present in the user data DataFrame.\n",
    "   - Extracts user indices, place indices, and interaction scores from the DataFrame.\n",
    "\n",
    "3. **Model Training**:\n",
    "   - Calls the `fit` method of the model with user indices and place indices as input features and interaction scores as the target variable.\n",
    "   - Specifies batch size and number of epochs for training.\n",
    "   - Splits the data into training and validation sets using a validation split of 0.2.\n",
    "\n",
    "4. **Model Saving**:\n",
    "   - Saves the trained model after completing all epochs.\n",
    "\n",
    "This function facilitates the training of the NCF model using user-place interaction data, enabling the model to learn to predict user preferences for different places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NCF Model\n",
    "def train_ncf_model(model, user_data, batch_size=32, epochs=5):\n",
    "    if 'user_index' not in user_data.columns or 'place_index' not in user_data.columns or 'score' not in user_data.columns:\n",
    "        raise ValueError(\"user_data must contain 'user_index', 'place_index', and 'score' columns.\")\n",
    "\n",
    "    user_indices = user_data['user_index'].values\n",
    "    place_indices = user_data['place_index'].values\n",
    "    interactions = user_data['score'].values\n",
    "\n",
    "    model.fit(\n",
    "        x=[user_indices, place_indices],\n",
    "        y=interactions,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    model.save('../.models/ncf_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a function to generate recommendations using an NCF model. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `input_features`: Dictionary containing input features for the recommendation query, including 'tags' and 'rating'.\n",
    "   - `model`: The trained NCF model used for generating recommendations.\n",
    "   - `place_data`: Pandas DataFrame containing information about places, including 'tags' and 'rating'.\n",
    "   - `k`: Number of recommendations to generate (default is 20).\n",
    "\n",
    "2. **Input Data Conversion**:\n",
    "   - Converts the input features into TensorFlow tensors suitable for model prediction.\n",
    "   - Constructs an input dictionary containing 'tags' and 'rating' tensors.\n",
    "\n",
    "3. **Query Embedding**:\n",
    "   - Uses the model to predict the embedding for the input query.\n",
    "\n",
    "4. **Location Embeddings**:\n",
    "   - Predicts embeddings for all places in the `place_data` DataFrame using the model.\n",
    "   - Converts tags to sequences and pads them for compatibility with the model input.\n",
    "\n",
    "5. **Similarity Calculation**:\n",
    "   - Computes cosine similarity between the query embedding and embeddings of all places.\n",
    "   - Determines the top k indices of places with the highest similarity scores.\n",
    "\n",
    "6. **Recommendations**:\n",
    "   - Returns the indices of the top k recommended places based on similarity scores.\n",
    "\n",
    "This function facilitates the generation of recommendations by finding places with embeddings most similar to the embedding of the input query, as predicted by the trained NCF model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Recommendations\n",
    "def recommend(input_features, model, place_data, k=20):\n",
    "    input_dict = {\n",
    "        'tags': tf.convert_to_tensor([input_features['tags']], dtype=tf.int32),\n",
    "        'rating': tf.convert_to_tensor([input_features['rating']], dtype=tf.float32)\n",
    "    }\n",
    "    \n",
    "    query_embedding = model.predict(input_dict)\n",
    "    location_embeddings = model.predict({\n",
    "        'tags': pad_sequences(place_data['tags'], padding='post'),\n",
    "        'rating': place_data['rating'].values\n",
    "    })\n",
    "    \n",
    "    similarities = cosine_similarity(query_embedding, location_embeddings)\n",
    "    top_k_indices = similarities[0].argsort()[-k:][::-1]\n",
    "    \n",
    "    return top_k_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a function to generate recommendations for a specific user using an NCF model. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `user_id`: Identifier of the user for whom recommendations are generated.\n",
    "   - `model`: The trained NCF model used for generating recommendations.\n",
    "   - `place_data`: Pandas DataFrame containing information about places.\n",
    "   - `user_id_to_index`: Dictionary mapping user IDs to their corresponding indices.\n",
    "   - `k`: Number of recommendations to generate (default is 10).\n",
    "\n",
    "2. **User ID Validation**:\n",
    "   - Checks if the provided user ID exists in the `user_id_to_index` dictionary. Raises a KeyError if not found.\n",
    "\n",
    "3. **User Index Retrieval**:\n",
    "   - Retrieves the index of the user from the `user_id_to_index` dictionary.\n",
    "\n",
    "4. **Place Indices Generation**:\n",
    "   - Creates an array of indices representing all places in the `place_data` DataFrame.\n",
    "   - Creates an array of user indices, with the user index repeated for each place.\n",
    "\n",
    "5. **Prediction and Sorting**:\n",
    "   - Predicts scores for all places for the specified user using the NCF model.\n",
    "   - Sorts the places based on predicted scores and selects the top k indices.\n",
    "\n",
    "6. **Top-k Recommendations**:\n",
    "   - Retrieves the top-k recommended places from the `place_data` DataFrame based on the sorted indices.\n",
    "\n",
    "This function facilitates the generation of recommendations for a specific user by predicting scores for all places and selecting the top-k recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncf_recommend(user_id, model, place_data, user_id_to_index, k=10):\n",
    "    if user_id not in user_id_to_index:\n",
    "        raise KeyError(f\"User ID {user_id} not found in user_id_to_index.\")\n",
    "    \n",
    "    user_index = user_id_to_index[user_id]\n",
    "    place_indices = np.arange(len(place_data))\n",
    "    user_indices = np.full(len(place_data), user_index)\n",
    "    \n",
    "    scores = model.predict([user_indices, place_indices]).flatten()\n",
    "    top_k_indices = scores.argsort()[-k:][::-1]\n",
    "    top_k_places = place_data.iloc[top_k_indices]\n",
    "    \n",
    "    return top_k_places\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines a function to generate recommendations using a weighted hybrid approach combining content-based and collaborative filtering methods. Here's an explanation of the function:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `input_features`: Dictionary containing input features for the recommendation query, including 'tags' and 'rating'.\n",
    "   - `user_id`: Identifier of the user for whom recommendations are generated.\n",
    "   - `content_model`: The trained content-based recommendation model.\n",
    "   - `cf_model`: The trained collaborative filtering (CF) model.\n",
    "   - `place_data`: Pandas DataFrame containing information about places.\n",
    "   - `user_id_to_index`: Dictionary mapping user IDs to their corresponding indices.\n",
    "   - `cb_weight`: Weight assigned to content-based recommendations (default is 0.5).\n",
    "   - `cf_weight`: Weight assigned to collaborative filtering recommendations (default is 0.5).\n",
    "   - `k`: Number of recommendations to generate (default is 10).\n",
    "   - `food_tags`: List of tags representing food-related categories (default is [13, 14, 15, 16]).\n",
    "\n",
    "2. **Content-Based Recommendations**:\n",
    "   - Generates content-based recommendations using the `recommend` function.\n",
    "   - Retrieves top k recommendations based on content similarity.\n",
    "\n",
    "3. **Collaborative Filtering Recommendations**:\n",
    "   - Generates collaborative filtering recommendations using the `ncf_recommend` function.\n",
    "   - Retrieves top k recommendations based on predicted scores from the CF model.\n",
    "\n",
    "4. **Combining Recommendations**:\n",
    "   - Concatenates content-based and collaborative filtering recommendations.\n",
    "   - Marks each recommendation with its source (content-based or collaborative filtering).\n",
    "\n",
    "5. **Normalization and Weighted Scores**:\n",
    "   - Normalizes scores obtained from both methods using MinMaxScaler.\n",
    "   - Calculates weighted scores based on the specified weights for content-based and collaborative filtering recommendations.\n",
    "\n",
    "6. **Sorting and Filtering**:\n",
    "   - Sorts recommendations based on weighted scores in descending order.\n",
    "   - Limits recommendations to top k places.\n",
    "\n",
    "7. **Food Tag Limitation**:\n",
    "   - Limits the number of food-tagged places to a maximum of 3.\n",
    "   - Ensures that the final recommendations list contains exactly k places.\n",
    "\n",
    "This function enables the generation of recommendations by combining content-based and collaborative filtering approaches with customizable weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_hybrid_recommend(input_features, user_id, content_model, cf_model, place_data, user_id_to_index, cb_weight=0.5, cf_weight=0.5, k=10, food_tags=[13,14,15,16]):\n",
    "    # Get content-based recommendations\n",
    "    cb_recommendations = recommend(input_features, content_model, place_data, k)\n",
    "    \n",
    "    # Get collaborative filtering recommendations\n",
    "    try:\n",
    "        cf_recommendations = ncf_recommend(user_id, cf_model, place_data, user_id_to_index, k)\n",
    "        cf_scores = cf_model.predict([np.full((len(place_data),), user_id_to_index[user_id]), place_data.index.values])\n",
    "        cf_scores = cf_scores.flatten()\n",
    "    except KeyError:\n",
    "        cf_recommendations = pd.DataFrame()\n",
    "        cf_scores = np.zeros(len(place_data))\n",
    "\n",
    "    # Combine and mark the recommendations\n",
    "    all_recommendations = pd.concat([place_data.iloc[cb_recommendations], cf_recommendations])\n",
    "    all_recommendations['source'] = ['content-based'] * len(cb_recommendations) + ['collaborative-filtering'] * len(cf_recommendations)\n",
    "\n",
    "    if 'place_id' not in all_recommendations.columns:\n",
    "        all_recommendations['place_id'] = all_recommendations['name'].apply(lambda name: place_data.loc[place_data['name'] == name, 'place_id'].iloc[0])\n",
    "\n",
    "    all_recommendations = all_recommendations.drop_duplicates('place_id')\n",
    "\n",
    "    # Normalize the scores\n",
    "    scaler = MinMaxScaler()\n",
    "    cb_recommendations = all_recommendations.index[all_recommendations['source'] == 'content-based']\n",
    "    cf_recommendations = all_recommendations.index[all_recommendations['source'] == 'collaborative-filtering']\n",
    "\n",
    "    all_recommendations['score'] = scaler.fit_transform(\n",
    "        np.concatenate([\n",
    "            cosine_similarity(content_model.predict({\n",
    "                'tags': tf.convert_to_tensor([input_features['tags']], dtype=tf.int32),\n",
    "                'rating': tf.convert_to_tensor([input_features['rating']], dtype=tf.float32)\n",
    "            }), content_model.predict({\n",
    "                'tags': pad_sequences([place_data.iloc[idx]['tags']], padding='post'),\n",
    "                'rating': np.array([place_data.iloc[idx]['rating']])\n",
    "            }))[0].reshape(-1, 1) for idx in cb_recommendations\n",
    "        ] + [cf_scores[cf_recommendations].reshape(-1, 1)])\n",
    "    )\n",
    "\n",
    "    # Calculate the weighted scores\n",
    "    all_recommendations['weighted_score'] = all_recommendations.apply(\n",
    "        lambda row: row['score'] * cb_weight if row['source'] == 'content-based' else row['score'] * cf_weight,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Sort and limit to top k recommendations\n",
    "    top_recommendations = all_recommendations.sort_values('weighted_score', ascending=False).drop_duplicates('place_id').head(k)\n",
    "\n",
    "    # Limit food-tagged places to a maximum of 3\n",
    "    food_mask = top_recommendations['tags'].apply(lambda tags: any(tag in food_tags for tag in tags))\n",
    "    food_places = top_recommendations[food_mask].head(3)\n",
    "    non_food_places = top_recommendations[~food_mask]\n",
    "\n",
    "    final_recommendations = pd.concat([non_food_places, food_places])\n",
    "\n",
    "    # Ensure the final recommendations list has exactly 10 places\n",
    "    if len(final_recommendations) < k:\n",
    "        remaining_slots = k - len(final_recommendations)\n",
    "        additional_non_food_places = all_recommendations[~all_recommendations.index.isin(final_recommendations.index) \n",
    "                                                         & ~all_recommendations['tags']\n",
    "                                                        .apply(lambda tags: any(tag in food_tags for tag in tags))].head(remaining_slots)\n",
    "        final_recommendations = pd.concat([final_recommendations, additional_non_food_places])\n",
    "    \n",
    "    return final_recommendations.head(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python cell defines the main script for recommending places to a user. Here's an explanation of the script:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Loads and preprocesses place data to prepare it for model training and recommendation.\n",
    "   - Extracts unique tags from the place data.\n",
    "\n",
    "2. **User Data Processing**:\n",
    "   - Loads historical user interaction data.\n",
    "   - Preprocesses user data by adding user and place indices.\n",
    "   - Creates dictionaries mapping user and place IDs to their corresponding indices.\n",
    "\n",
    "3. **Model Training (NCF)**:\n",
    "   - Creates an NCF model using the number of unique users and places.\n",
    "   - Trains the NCF model using historical user interaction data.\n",
    "\n",
    "4. **Model Loading**:\n",
    "   - Loads the pre-trained Triplet and NCF models.\n",
    "\n",
    "5. **Input Features Generation**:\n",
    "   - Randomly generates input features for recommendation, including tags and rating.\n",
    "\n",
    "6. **Recommendation Generation**:\n",
    "   - Calls the `weighted_hybrid_recommend` function to generate recommendations.\n",
    "   - Passes the input features along with the Triplet and NCF models, user data, and other parameters.\n",
    "   - Prints the recommended places for the specified user ID.\n",
    "\n",
    "7. **Return Value**:\n",
    "   - Returns the list of recommended place IDs.\n",
    "\n",
    "This script orchestrates the entire process of recommending places to a user, including data preprocessing, model training, input feature generation, recommendation generation, and output display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8478 - loss: 0.5444 - val_accuracy: 0.8896 - val_loss: 0.3513\n",
      "Epoch 2/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9085 - loss: 0.2486 - val_accuracy: 0.7787 - val_loss: 0.5095\n",
      "Epoch 3/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0068 - val_accuracy: 0.5220 - val_loss: 1.4006\n",
      "Epoch 4/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.4760e-04 - val_accuracy: 0.4850 - val_loss: 1.6962\n",
      "Epoch 5/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.0265e-04 - val_accuracy: 0.4915 - val_loss: 1.7989\n",
      "Input features:\n",
      "Tags: [22, 0, 16, 6, 19]\n",
      "Rating: 4.5\n",
      "Recommending places for user with id: 1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "                             name  rating  place_id                      tags  \\\n",
      "94               Maslak Pavilions     4.6    129727     [1, 3, 4, 7, 0, 0, 0]   \n",
      "96                Rumeli Fortress     4.5    528752     [1, 3, 4, 7, 0, 0, 0]   \n",
      "55      Abdülmecid Efendi Mansion     4.4    424824     [1, 3, 4, 7, 0, 0, 0]   \n",
      "91               Besiktas Stadium     4.7    676421  [18, 19, 20, 0, 0, 0, 0]   \n",
      "102            Galatasaray Museum     4.6    615371  [18, 19, 20, 0, 0, 0, 0]   \n",
      "138  Fatih Sultan Mehmet Monument     4.3    755016     [1, 3, 4, 6, 0, 0, 0]   \n",
      "90               Ihlamur Pavilion     4.6    925873     [1, 2, 3, 4, 7, 0, 0]   \n",
      "51              Beylerbeyi Palace     4.7    140042     [1, 2, 3, 4, 7, 0, 0]   \n",
      "44                   Maiden Tower     4.7    286657     [1, 2, 3, 4, 7, 0, 0]   \n",
      "88                 Çırağan Palace     4.7    152392     [1, 2, 3, 4, 7, 0, 0]   \n",
      "\n",
      "                                locationYX         source     score  \\\n",
      "94     41.12019630844922,29.02533600745049  content-based  1.000000   \n",
      "96    41.08515779378035,29.056979680272512  content-based  1.000000   \n",
      "55   41.028306057428004,29.041573903075108  content-based  0.999999   \n",
      "91   41.039995299764044,28.994412698597092  content-based  0.999988   \n",
      "102   41.10354479923963,28.990814351439013  content-based  0.999987   \n",
      "138     41.03295177150576,28.9365025799525  content-based  0.999977   \n",
      "90    41.05195756244766,29.001298861846962  content-based  0.999970   \n",
      "51   41.042612743417656,29.040053076966498  content-based  0.999965   \n",
      "44                    41.021167,29.00425\\t  content-based  0.999965   \n",
      "88    41.04387308576512,29.015929923862068  content-based  0.999965   \n",
      "\n",
      "     weighted_score  \n",
      "94         0.800000  \n",
      "96         0.800000  \n",
      "55         0.799999  \n",
      "91         0.799990  \n",
      "102        0.799990  \n",
      "138        0.799981  \n",
      "90         0.799976  \n",
      "51         0.799972  \n",
      "44         0.799972  \n",
      "88         0.799972  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[129727,\n",
       " 528752,\n",
       " 424824,\n",
       " 676421,\n",
       " 615371,\n",
       " 755016,\n",
       " 925873,\n",
       " 140042,\n",
       " 286657,\n",
       " 152392]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main script\n",
    "def recommend_places(user_id):\n",
    "    # # Preprocess data\n",
    "    whole_data, unique_tags = preprocess_data('../../../data/place_data.csv')\n",
    "    \n",
    "    # # Create and train triplet model\n",
    "    # triplet_model = create_triplet_model(unique_tags)\n",
    "    # anchor_features, positive_features, negative_features = generate_triplets(whole_data)\n",
    "    # train_triplet_model(triplet_model, anchor_features, positive_features, negative_features)\n",
    "    \n",
    "    # Load user data\n",
    "    user_data = pd.read_csv('../../../data/historical/historical_interactions.csv')\n",
    "    \n",
    "    # Preprocess user_data\n",
    "    user_data['user_id'] = user_data.index\n",
    "    unique_user_ids = user_data['user_id'].unique()\n",
    "    unique_place_ids = user_data['place_id'].unique()\n",
    "    user_id_to_index = {user_id: index for index, user_id in enumerate(unique_user_ids)}\n",
    "    place_id_to_index = {place_id: index for index, place_id in enumerate(unique_place_ids)}\n",
    "    user_data['user_index'] = user_data['user_id'].map(user_id_to_index)\n",
    "    user_data['place_index'] = user_data['place_id'].map(place_id_to_index)\n",
    "    \n",
    "    num_users = user_data['user_index'].nunique()\n",
    "    num_places = user_data['place_index'].nunique()\n",
    "    \n",
    "    # # Create and train NCF model\n",
    "    ncf_model = create_ncf_model(num_users, num_places)\n",
    "    train_ncf_model(ncf_model, user_data)\n",
    "    \n",
    "    # Load trained models\n",
    "    triplet_model = load_model('../.models/triplet_model.keras', custom_objects={'triplet_loss': triplet_loss})\n",
    "    ncf_model = load_model('../.models/ncf_model.keras')\n",
    "    \n",
    "    # Randomly generate input features for recommendation. Tags are between 1 and 14 at least 3 many, rating is between 1 and 5.\n",
    "    tags = np.random.choice(unique_tags, size=5, replace=False).tolist()\n",
    "    rating = 4.5\n",
    "\n",
    "    print(\"Input features:\")\n",
    "    print(\"Tags:\", tags)\n",
    "    print(\"Rating:\", rating)\n",
    "\n",
    "    input_features = {\n",
    "        'tags': tags,\n",
    "        'rating': rating\n",
    "    }\n",
    "\n",
    "    print(\"Recommending places for user with id:\", user_id)\n",
    "\n",
    "    recommended_items = weighted_hybrid_recommend(input_features, user_id, triplet_model, ncf_model, whole_data, user_id_to_index, cb_weight=0.8, cf_weight=0.05, k=10)\n",
    "    print(recommended_items)\n",
    "\n",
    "    return recommended_items[\"place_id\"].tolist()\n",
    "\n",
    "recommend_places(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
